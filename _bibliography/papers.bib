---
---
@article{jung2023mo2hap,
    title     = {Mo2Hap- Rendering VR Performance Motion Flow to Upper-body Vibrotactile Haptic Feedback},
    author    = {Jung, Kyungeun and Yoon, Sang Ho},
    journal   = {UIST (demo)},
    year      = {2023},
    abstract  = {Semantic segmentation for autonomous driving should be robust against various in-the-wild environments. Nighttime semantic segmentation is especially challenging due to a lack of annotated nighttime images and a large domain gap from daytime images with sufficient annotation. In this paper, we propose a novel GPS-based training framework for nighttime semantic segmentation. Given GPS-aligned pairs of daytime and nighttime images, we perform cross-domain correspondence matching to obtain pixel-level pseudo supervision. Moreover, we conduct flow estimation between daytime video frames and apply GPS-based scaling to acquire another pixel-level pseudo supervision. Using these pseudo supervisions with a confidence map, we train a nighttime semantic segmentation network without any annotation from nighttime images. Experimental results demonstrate the effectiveness of the proposed method on several nighttime semantic segmentation datasets.},
    abbr      = {UIST},
    code      = {https://github.com/jimmy9704/GPS-GLASS},
    selected  = {true},
    arxiv     = {2207.13297},
    img_path  = {assets/img/UIST.png}
}

@article{yoo2023vosvfi,
    title     = {Video Object Segmentation-aware Video Frame Interpolation},
    author    = {Yoo, Jun-Sang and Lee, Hongjae and Jung, Seung-Won},
    journal   = {ICCV},
    year      = {2023},
    abstract  = {Video frame interpolation (VFI) is a very active research topic due to its broad applicability to many applications, including video enhancement, video encoding, and slow-motion effects. VFI methods have been advanced by improving the overall image quality for challenging sequences containing occlusions, large motion, and dynamic texture. This mainstream research direction neglects that foreground and background regions have different importance in perceptual image quality. Moreover, accurate synthesis of moving objects can be of utmost importance in computer vision applications. In this paper, we propose a video object segmentation (VOS)-aware training framework called VOS-VFI that allows VFI models to interpolate frames with more precise object boundaries. Specifically, we exploit VOS as an auxiliary task to help train VFI models by providing additional loss functions, including segmentation loss and bi-directional consistency loss. From extensive experiments, we demonstrate that VOS-VFI can boost the performance of existing VFI models by rendering clear object boundaries. Moreover, VOS-VFI displays its effectiveness on multiple benchmarks for different applications, including video object segmentation, object pose estimation, and visual tracking.},
    abbr      = {ICCV},
    selected  = {true},
    img_path  = {assets/img/vos_vfi.png}
}
@article{yoo2022hst,
    title     = {Hierarchical Spatiotemporal Transformers for Video Object Segmentation},
    author    = {Yoo, Jun-Sang and Lee, Hongjae and Jung, Seung-Won},
    journal   = {ICCVW (oral)},
    year      = {2023},
    abstract  = {This paper presents a novel framework called HST for semi-supervised video object segmentation (VOS). HST extracts image and video features using the latest Swin Transformer and Video Swin Transformer to inherit their inductive bias for the spatiotemporal locality, which is essential for temporally coherent VOS. To take full advantage of the image and video features, HST casts image and video features as a query and memory, respectively. By applying efficient memory read operations at multiple scales, HST produces hierarchical features for the precise reconstruction of object masks. HST shows effectiveness and robustness in handling challenging scenarios with occluded and fast-moving objects under cluttered backgrounds. In particular, HST-B outperforms the state-of-the-art competitors on multiple popular benchmarks.},
    abbr      = {ICCVW},
    selected  = {true},
    arxiv     = {2307.08263},
    img_path  = {assets/img/hst.png}
}
@article{lee2022phomonet,
    title     = {Monocular Depth Estimation Network with Single-Pixel Depth Guidance},
    author    = {Lee, Hongjae and Park, Jinbum and Wooseok Jeong and Jung, Seung-Won},
    journal   = {Optics Letters},
    year      = {2022},
    abstract  = {Due to the scale ambiguity problem, the performance  of monocular depth estimation (MDE) is inherently restricted. Multi-camera systems, especially those equipped with active depth cameras, have addressed this problem at the expense of increased hardware costs and space. In this Letter, we adopt a similar but costeffective solution using only single-pixel depth guidance with a single-photon avalanche diode. To this end, we design a single-pixel guidance module (SPGM) that combines the global information from the single-pixel depth guidance with the spatial information from the image at the feature level. By integrating SPGMs into an MDE network, we introduce PhoMoNet, the first end-to-end MDE network with single-pixel depth guidance. Experimental results show the effectiveness and superiority of PhoMoNet over state-of-the-art MDE networks on synthetic and real-world datasets.},
    abbr      = {Optics},
    code      = {https://github.com/jimmy9704/PhoMoNet},
    selected  = {true},
    img_path  = {assets/img/phomonet.png}
}
@article{lee2021plane,
    title     = {Clustering-based plane segmentation neural network for urban scene modeling},
    author    = {Lee, Hongjae and Jung, Jiyoung},
    journal   = {Sensors},
    volume    = {21},
    number    = {24},
    pages     = {8382},
    year      = {2021},
    abstract  = {Urban scene modeling is a challenging but essential task for various applications, such as 3D map generation, city digitization, and AR/VR/metaverse applications. To model man-made structures, such as roads and buildings, which are the major components in general urban scenes, we present a clustering-based plane segmentation neural network using 3D point clouds, called hybrid K-means plane segmentation (HKPS). The proposed method segments unorganized 3D point clouds into planes by training the neural network to estimate the appropriate number of planes in the point cloud based on hybrid K-means clustering. We consider both the Euclidean distance and cosine distance to cluster nearby points in the same direction for better plane segmentation results. Our network does not require any labeled information for training. We evaluated the proposed method using the Virtual KITTI dataset and showed that our method outperforms conventional methods in plane segmentation. Our code is publicly available.},
    abbr      = {Sensors},
    code      = {https://github.com/jimmy9704/plane-segmentation-network},
    earlyaccess={false},
    selected  = {true},
    img_path={assets/img/plane_segmentation.png}
}
